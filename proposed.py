import torch
from DP.DP_model import SGM, ConvAttackModel
from DP.my_attacker import get_std, MyAttacker
import pandas as pd
from DP_util import get_samples, attack_cfg_default, data_cfg_default, get_clipping_bound
from metrics import cw_ssim, psnr_compute, mse_compute


def perform_attacks(save_fig=True, data_cfg=data_cfg_default(), attack_cfg=attack_cfg_default(), fig_idx=None,
                    use_dp=True, clipping_bound=100, dataset='ImageNet', epsilon=100):
    # generate grads with random samples, and recovered the samples from grads
    device = data_cfg.device
    setup = dict(device=device, dtype=torch.float)

    batch_size = data_cfg.batch_size
    # fig_idx = [100 * i for i in range(1, batch_size+1)]
    if fig_idx is None:
        fig_idx = torch.randint(0, 10000, (batch_size,))
    datapoint, labels = get_samples(fig_idx, device=device, dataset=dataset)

    # This could be any model:
    loss_fn = torch.nn.CrossEntropyLoss()
    # It will be modified maliciously:

    # model = SGM(attack_cfg=attack_cfg, data_cfg=data_cfg, gain=1).to(device)
    model = ConvAttackModel(attack_cfg=attack_cfg, data_cfg=data_cfg, gain=1).to(device)
    secret = dict(weight_idx=0, bias_idx=1, shape=tuple(data_cfg.shape), structure=model.structure, gain=1)
    secrets = {"ImprintBlock": secret}

    # This is the attacker:
    attacker = MyAttacker(model, loss_fn, attack_cfg, data_cfg, setup)

    # ## Simulate an attacked FL protocol
    # Server-side computation:
    server_payload = [
        dict(
            parameters=[p for p in model.parameters()], buffers=[b for b in model.buffers()], metadata=data_cfg
        )
    ]
    # User-side computation:
    loss = loss_fn(model(datapoint), labels)
    shared_data = [
        dict(
            gradients=torch.autograd.grad(loss, model.parameters()),
            buffers=None,
            metadata=dict(num_data_points=len(fig_idx), labels=labels, local_hyperparams=None,),
        )
    ]

    if use_dp:
        # clipping and add noise to gradients
        grad_w = []
        std = get_std(clipping_bound=clipping_bound, epsilon=epsilon)
        for grad in shared_data[0]['gradients']:
            grad_w.append(torch.norm(grad) ** 2)
        grad_norm = sum(grad_w) ** 0.5

        clipping_factor = grad_norm / clipping_bound
        for grad in shared_data[0]['gradients']:
            if grad_norm > clipping_bound:
                grad /= clipping_factor  # clipping
            grad += torch.normal(mean=0, std=std, size=grad.size())  # add noise

    # Attack:
    # reconstructed_user_data, stats = attacker.conv_reconstruct(server_payload, shared_data, clipping_bound, 0)
    reconstructed_user_data = attacker.conv_reconstruct(server_payload, shared_data, clipping_bound, 0)

    # the raw recovered results
    image_res = reconstructed_user_data['data']
    torch.save(image_res, r'intermediate results/raw res.pt')

    #  To distinguish the complete images, overlapped images and meaningless images
    last_non_zero_idx = torch.tensor(torch.load(r'intermediate results/last non zero idx.pt')).to(device)
    separated_input, separated_res = [], []
    overlapped_input, overlapped_res = dict(), dict()
    meaningless_res = []
    valid_image_bias_idx = torch.load('intermediate results/res non zero idx.pt')  # res non zero idx in bias
    for res_idx, res_bias_idx in enumerate(valid_image_bias_idx):
        if res_bias_idx in last_non_zero_idx:
            # not a meaningless image, maybe a separated or overlapped image
            if len(torch.where(last_non_zero_idx == res_bias_idx)[0]) == 1:
                # is a separated input
                separated_input.append(datapoint[torch.where(last_non_zero_idx == res_bias_idx)].squeeze(dim=0))
                separated_res.append(image_res[res_idx])
            else:
                # is an overlapped image
                if res_bias_idx not in overlapped_input:
                    # first time occurs
                    # multiples separated images
                    overlapped_input[res_bias_idx] = datapoint[torch.where(last_non_zero_idx == res_bias_idx)]
                    # one overlapped image
                    overlapped_res[res_bias_idx] = image_res[res_idx].unsqueeze(dim=0)
        else:
            # meaningless images generated by the noise
            meaningless_res.append(image_res[res_idx])

    if save_fig:
        # save the training samples and recovered samples locally
        print('Batch size:', batch_size)
        print('Raw res:', len(image_res))
        print('Separated image:', len(separated_res))
        print('Overlapped image:', len(overlapped_res))
        print('Meaningless image:', len(meaningless_res))

        torch.save(datapoint, r'intermediate results/original data.pt')
        # torch.save(image_res, r'intermediate results/raw res.pt')
        # torch.save(overlapped_input, r'intermediate results/overlapped input.pt')
        # torch.save(overlapped_res, r'intermediate results/overlapped res.pt')
        # torch.save(separated_input, r'intermediate results/separated input.pt')
        torch.save(separated_res, r'intermediate results/separated res.pt')
        # torch.save(meaningless_res, r'intermediate results/meaningless res.pt')

    return image_res, separated_res, separated_input, overlapped_res, overlapped_input, meaningless_res


if __name__ == "__main__":
    cb = get_clipping_bound('resnet', 'resnet101', False)
    dc = data_cfg_default()
    ac = attack_cfg_default()
    ac.compress_image = False
    record = pd.DataFrame(columns=['batch size', 'bin num', 'res num', 'separated res', 'separated input',
                                   'overlapped res', 'overlapped input', 'meaningless'])
    for bs in range(1, 2):
        dc.batch_size = int(2 ** bs)
        for rounds in range(1):
            fig_idx = torch.tensor([1200, 17000, 10696, 4800, 16001, 18000, 9600, 10800, 1201, 17001, 10697, 4801,
                                    16002, 18001, 9601, 10801])
            # fig_idx = torch.randint(0, 10000, (dc.batch_size,))
            for bin_num in (128,):
                ac.num_bins = bin_num
                print('batch size: {0}, bins num: {1}, round {2}'.format(dc.batch_size, ac.num_bins, rounds))

                res = perform_attacks(save_fig=True, data_cfg=dc, attack_cfg=ac,
                                      clipping_bound=cb, use_dp=False)
                '''res = perform_attacks(save_fig=True, data_cfg=dc, fig_idx=fig_idx[:dc.batch_size], attack_cfg=ac,
                                      clipping_bound=cb, use_dp=True)'''
                record.loc[len(record)] = [dc.batch_size, ac.num_bins, *list(map(len, res))]

                accuracy = len(res[1]) / dc.batch_size
                separated_res = torch.stack(res[1])
                separated_input = torch.stack(res[2])
                ssim = cw_ssim(separated_res, separated_input)[0]
                psnr = psnr_compute(separated_res, separated_input)[0]
                mse = mse_compute(separated_res, separated_input)

    # record.to_json(r'performance/record.json')
